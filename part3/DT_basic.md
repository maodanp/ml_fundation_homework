## 决策树基础
### 决策树学习
决策树的算法通常是一个递归的选择最有特征，并根据该特征对训练数据进行分割，使得各个数据集有一个最好的分类的过程，这一过程对应着对特征空间的划分，也对应着决策树的构建。

开始，构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建对应节点，如此递归进行，直到所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分分到叶节点上。这就生成了一颗决策树。

但是以上方法对于为止的测试数据未必有很好的分类能力，即可能发生过拟合现象，我们需要对已经生成的树自下而上进行剪枝，将树变得更简单，从而使得它具有更好的泛华能力。

决策树学习算法包含特征选择、决策树生成与决策树剪枝的过程。

### 特征选择（信息论）
信息论中熵度量了事物的不确定性，越不约定的事物，熵就越大。
$$
H(X) = -\sum_{i=1}^np_ilogp_i
$$
当$$p=0.5$$时，$$H(p)=1$$，熵取值最大，随机变量不确定性最大。

多变量**联合熵**，这里给出X和Y的联合熵表达式：
$$
H(X,Y)=-\sum_{i=1}^np(x_i,y_i)logp(x_i, y_i)
$$
条件熵，度量我们在知道Y后X剩下的不确定性：
$$
H(X|Y)=-\sum_{i=1}^np(x_i|y_i)logp(x_i, y_i)=\sum_{j=1}^np(y_i)H(X|y_i)
$$

#### 信息增益
熵$$H(Y)$$与条件熵$$H(Y|X)$$之差称为互信息，等价于决策树中的信息熵。
$$
g(D, A) = H(D) - H(D|A)
$$
表示特征A对训练数据D的信息增益。表示由于由于特征A而使得对数据集D的分类的不确定性减少的程度。
> 显然，对于数据集D而言信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。

#### 信息增益比
信息增益值的大小相对于训练数据集而言的，并没有绝对意义。也就是说在训练数据集的经验熵大的时候，信息增益值会偏大，反之信息增益值会偏小。
$$
g_R(D,A) = \frac{g(D, A)}{H(D)}
$$

### ID3 算法
#### 算法流程
1) 初始化信息增益的阈值$$\varepsilon$$

2）判断样本是否为同一类输出$$D_i$$，如果是则返回单节点树T。标记类别为$$D_i$$

3) 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。

4）计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征$$A_g$$

5) 如果$$A_g$$的信息增益小于阈值$$\varepsilon$$，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。

6）否则，按特征$$A_g$$的不同取值$$A_{gi}$$将对应的样本输出D分成不同的类别$$D_i$$。每个类别产生一个子节点。对应特征值为$$A_{gi}$$。返回增加了节点的数T。

7）对于所有的子节点，令$$D=D_i,A=A−{A_g}$$递归调用2-6步，得到子树$$T_i$$并返回。

#### ID3不足
1) ID3没有考虑连续特征，大大限制ID3的用途

2) ID3采用信息增益大的特征优先建立决策树的节点，则在相同条件下取值比较多的特征比取值少的特征信息增益大。

3) 缺失值未考虑

4) 未考虑过拟合。

### C4.5算法

#### 如何处理连续特征？

C4.5是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为$$a_1, a_2,\cdots,a_m$$，则该算法取相邻两样本值的平均数，一共取得m-1个划分点。对于这m-1个点，分别计算以改点作为二元分类点的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。

例如取到的增益最大的点为$$a_t$$，则小于$$a_t$$的值为类别1， 大于的值为类别2，这样就做到了连续特征的离散化。与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

#### 如何处理特征信息增益的偏向性

信息增益作为标准容易偏向取值较多的特征的问题，我们引入一个信息增益率的变量，它是信息增益和特征熵的比值。表达式如下：
$$
g_R(D,A) = \frac{g(D, A)}{H(D)}
$$

#### 如何处理缺失值问题

主要需要解决两个问题，一个是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在改属性上缺失特征的样本的处理。

对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

#### 如何处理过拟合

C4.5引入了正则化系数进行初步的剪枝。

#### C4.5不足

C.5虽然改善了ID3算法的几个主要问题，但是仍然有优化的空间。

1） C4.5的兼职方法有优化的空间，思路有两种，一个是预剪枝，即在生成决策树的时候就决定是否剪枝；另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。CART树的剪枝主要采用的是后简直加上交叉验证选择最合适的决策树。

2）C4.5生成的是多叉树，即一个父节点可能有多个节点，可以采用二叉树模型，相对而言运算效率更高。

3）C4.5只能用于分类，如果能够将决策树用于回归，则可以扩大它的使用范围。

4）C4.5由于使用了熵模型，内含耗时的对数运算，如果是连续值还有大量的排序运算，如果能够加以模型简化可以减少运算强度。

这四个问题在CART中加以改进了，所以如果不考虑集成学习(ensemble learning)，在普通的决策树算法里，CART算法算是比较优的算法了。



### 参考阅读

<统计学习方法> 第五章 决策树

[决策树算法原理](http://www.cnblogs.com/pinard/p/6050306.html)





