## CART

CART同样由特征选择，树的生成及剪枝组成，既可以用于分类也可以用于回归。

CART算法由以下两步组成：

1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大

2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

### CART分类树

#### 最优特征选择方法

Gini指数，假设有K个类，样本点属于第k类的概率为$$p_k$$, 则概率分布的基尼指数定义为：
$$
Gini(p) = -\sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2
$$
可以看做$$f(x) = -lnx$$在$$x=1$$处的泰勒一阶展开，忽略告诫无穷小，得到$$f(x) \approx 1-x$$。
$$
H(X) = -\sum_{k=1}^Kp_klnp_k \approx -\sum_{k=1}^Kp_k(1-p_k)
$$
对于二分类问题，若样本点属于第1个类的概率为p,则概率分布的基尼指数为：
$$
Gini(p) = 2p(1-p)
$$
如果样本集D根据特征A是否取得某一可能值被分割为$$D_1, D_2$$两个部分，则在特征A条件下，集合D的基尼系数定义为：
$$
Gini(D, A) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
基尼系数可以作为熵模型的一个近似替代；同时为进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样建立起来的是二叉树，而不是多叉树，这样可以进一步简化基尼系数的计算。

#### 对连续、离散特征的改进

对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。

对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。ID3和C4.5，如果某个特征A被选取建立决策树，如果有A1,A2,A3三种类型，则会在决策树上建立一个三叉的节点；而CART分类树使用的方法不同，采用不停的二分。

还是这个例子，CART分类树会考虑把A分成$$\{A1\}和\{A2,A3\}$$, $$\{A2\}和\{A1,A3\}, \{A3\}和\{A1,A2\}$$三种情况，找到基尼系数最小的组合，比如$$\{A2\}和\{A1,A3\}$$,然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是$${A1,A3}$$对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，**在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立**。

### CART回归树

CART回归树和CART分类树的建立算法大部分是类似的，只讨论它们的建立算法不同的地方。**回归数、分类树两者区别在于样本输出，如果样本输出是离散值，那么就是一颗分类树；如果样本输出是连续值，那么是一颗回归树**。

除了概念不同，CART回归树和CART分类树的建立和预测的区别主要有以下两点：

1）连续值的处理方法不同

2）决策树建立后做预测的方式不同

#### 连续值处理方法

CART分类树采用的是基尼系数的大小来度量特征的各个划分点的优劣情况，比较适合分类模型。但是对于回归模型，使用了常见的平方误差最小化准则。

CART回归树度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，表达式：
$$
\underbrace{min}_{A,s}[\underbrace{min}_{c_1}\sum_{x_i \in D_1 (A, s)}(y_i-c_1)^2+\underbrace{min}_{c_2}\sum_{x_i \in D_2 (A, s)}(y_i-c_2)^2]
$$
其中，$$c_1$$为D1样本集的样本输出均值，$$c_2$$为D2样本集的样本输出均值

#### 预测方法

CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别，而回归树输出不是类别，**它采用最终叶子的均值或者中位数来预测输出结果**。

### CART剪枝

#### 剪枝的损失函数

由于决策时算法容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的返回能力。

* 预剪枝：限制树深度，叶子节点个数，叶子节点样本数，信息增益量等。
* 后剪枝：通过一定的衡量标准$$C_\alpha(T) = C(T) + \alpha \cdot |T_{leaft}|$$

CART使用后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝后的树作为最终的CART。

在剪枝过程中，计算子树的损失函数：
$$
C\alpha(T) = C(T) + \alpha \cdot |T|
$$
其中，T为任意子树，C(T)为对训练数据的预测误差($$C(T) = \sum_{t \in leaf}N_t \cdot H(t)$$). |T|为子树的叶节点个数，$$\alpha \ge 0$$为参数。

剪枝后的误差/单根节点的误差$$C(T)$$必然大于剪枝前的误差$$C(T_t)$$。

* 当$$\alpha=0$$时，未剪枝的决策树损失最小；
* 当$$\alpha=+\infty$$时，单根节点的决策树损失最小。

即一般说来，$$\alpha$$越大，则剪枝剪得越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的$$\alpha$$，一定存在使得损失函数$$C_{\alpha}(T)$$最小的唯一子树。

剪枝思路，对于位于节点t的任意一颗子树$$T_t$$，如果没有剪枝，它的损失函数是：
$$
C\alpha(T_t) = C(T_t) + \alpha \cdot |T_t|
$$
如果将其剪枝掉，仅仅保留根节点，则损失是：
$$
C\alpha(T) = C(T) + \alpha
$$
当$$\alpha=0$$或者很小时，$$C_{alpha}(T_t)=C_{\alpha}(T)$$，当$$\alpha$$继续增大，并且当满足：
$$
\alpha=\frac{C(T)-C(T_t)}{|T_t-1|}
$$
此时，$$T_t和T$$有相同的损失函数，但是T节点跟梢，因此可以对子树$$T_t$$进行剪枝，变成一个叶子节点T。

CART树的价差验证测量，上面基于每个子树是否剪枝的阈值$$\alpha$$,如果我们把所有的节点是否剪枝的值$$\alpha$$都计算出来，然后分别针对不同的$$\alpha$$所对应的剪枝后的最优子树做交叉验证，这样就可以选择一个最好的$$\alpha$$，基于次就可以用对应的最优子树作为最终的结果。

#### 剪枝算法

输入是CART树建立算法得到的原始决策树$$T$$, 输出是最优决策子树$$T_{\alpha}$$。

1）初始化$$\alpha_{min}=\infty$$, 最优子树集合 $$w={T}$$

2）从叶子节点开始自下而上的计算各个内部节点t的训练误差损失函数$$C_{\alpha}(T_t)$$ (回归树为MSE, 分类树为基尼系数)，叶子节点树$$|T_t|$$，以及阈值$$\alpha=min\{\frac{C(T)-C(T_t)}{|T_i|-1}, \alpha_{min}\}$$, 更新$$\alpha_{min}=\alpha$$

3）得到所有节点的$$\alpha$$值的集合M

4）从M中选择最大的值$$\alpha_k$$，自上而下的访问子树t的内部节点，如果有$$\frac{C(T)-C(T_t)}{|T_i|-1} \le \alpha_k$$时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的额均值。这样得到的$$\alpha_k$$对应的最优子树$$T_k$$

5）最优子树集合$$w=w \bigcup T_k, M=M-{\alpha_k}$$

6）如果M不为空，则回到步骤4，否则就已经得到了所有的可选最优子树集合w

7）采用交叉杨峥在w选择最优子树$$T_{\alpha}$$

### 决策树算法小结

| 算法   | 支持模型  | 树结构  | 特征选择     | 连续值处理 | 缺失值处理 | 剪枝   |
| ---- | ----- | ---- | -------- | ----- | ----- | ---- |
| ID3  | 分类    | 多叉树  | 信息增益     | 不支持   | 不支持   | 不支持  |
| C4.5 | 分类    | 多叉树  | 信息增益比    | 支持    | 支持    | 支持   |
| CART | 分类、回归 | 二叉树  | 基尼系数、均方差 | 支持    | 支持    | 支持   |

缺点：

1）大多数，分类决策不应该由某一个特征决定的，而是应该由一组特征决定的，这样得到的决策树更加精确，这个决策树称之为多变量决策树（multi-variate decision tree）。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策，这个算法的代表是OC1。

2）如果样本发生一点点改动，就会导致树结构的剧烈改变，这个可以通过集成学习里面的随机森林之类的方法解决。

### 参考阅读

<统计学习方法> 第五章 决策树

[决策树算法原理](http://www.cnblogs.com/pinard/p/6053344.html)