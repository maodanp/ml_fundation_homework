##    随机森林

### Bagging

#### Bagging原理

Bagging采用有放回的随机采样，这样得到的采样集和训练集的样本个数相同，但样本内容不同。如果我们对有m个样本训练集做T次随机采样，则由于随机性，T个采样集各不相同。

*这和GBDT的子采样不同，GBDT的子采样是无放回采样，而Bagging的子采样是放回采样*

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$$\frac{1}{m}$$。不被采集到的概率为$$1−\frac{1}{m}$$。如果m次采样都没有被采集中的概率是$$(1−\frac{1}{m})^m$$。当$$m>\infty$$时，$$(1−\frac{1}{m})^m \approx 0.368$$。也就是说，每次约有36.79%的样本不会出现在Bootstrap所采集的样本集合中。

> 将未参与模型训练的数据称为袋外数据OOB(Out Of Bag)，可以用于取代测试集用于误差估计

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是**决策树和神经网络**。

#### Bagging算法流程

输入为样本集$$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\}$$，弱学习器算法, 弱分类器迭代次数T。

输出为最终的强分类器$$f(x)$$

1）对于$$t=1,2,\cdots,T$$:

​	1.a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$$D_m$$

​	1.b)用采样集$$D_m$$训练第m个基模型$$G_m(x)$$

2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

### 随机森林

随机森林相比于Bagging算法不同点：

1. Random forest使用CART决策树作为基模型；
2. 在使用决策树基础上，RF对决策树的建立做了改进，加入了特征的随机选择。

对于普通决策树，会在所有n个样本特征中选择一个最优特征作为决策树的节点，并依据此节点进行左右子树的划分，但是RF通过随机选择一部分样本特征$$n_{sub}$$，然后在这些随机选择的$$n_{sub}$$样本特征中，选择一个最优特征作为决策树左右子树的划分，这进一步增加了模型的泛化能力。

但是如果$$n_sub$$越小，则模型越健壮，当然此时对于训练集的拟合程度会变差，bias会变大。在实际案例中，一般会通过交叉杨峥调参获取一个合适的$$n_{sub}$$值。

#### 随机森林算法流程

输入为样本集$$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\}$$，弱学习器算法, 弱分类器迭代次数T。

输出为最终的强分类器$$f(x)$$

1）对于$$t=1,2,\cdots,T$$:

​	1.a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$$D_m$$

​	1.b)用采样集$$D_m$$训练第m个决策树模型$$G_m(x)$$，在训练决策树模型的节点是，在节点上所有样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分

2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

#### 随机森林小结

RF的主要优点有：

1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。

2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。

3） 在训练后，可以给出各个特征对于输出的重要性

4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。

5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。

6） 对部分特征缺失不敏感。

RF的主要缺点有：

1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。

2)  取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

## 参考阅读

[Bagging与随机森林算法原理小结](http://www.cnblogs.com/pinard/p/6156009.html)