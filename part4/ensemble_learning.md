## 集成学习理论

### 集成学习之学习框架

#### Bagging

从训练集进行随机采样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合，以产生最终的预测结果：

![](part4_1_bagging.jpg)

这里随机采样一般采用Boostrap sampling(自助采样法)，由于是随机采样，这样的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

随机森林是bagging的一个特化进阶版，所谓特化指随机森林的弱学习都是决策树；所谓进阶是随机森林在bagging的样本随机采样基础上，又加上了**特征的随机选择**。即对样本（输入数据的行），以及特征（输入数据的列）都进行采样。

#### Boosting

![](part4_1_boosting.jpg)

Boosting算法工作机制是先从训练集用初始权重训练处一个弱学习器1/分类器/基模型，根据弱学习器的误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视，然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器达到事先指定的数目T，最终这T个弱学习器通过结合策略进行整合，得到最终的强学习器。

Boosting系列算法中最著名的算法有**AdaBoost算法**和**提升树(Boosting tree)**系列算法。提升树系列算法里面应用最广泛的是**梯度提升树GBDT(Gradient Boosting Decision Tree)**。

#### Stacking

将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测：

![](part4_1_stacking.jpg)

### 偏差与方差

我们通常说集成学习框架中基模型是弱模型，通常来说**弱模型是偏差高、方差小的模型**。但是并不是所有集成学习框架中的基模型都是弱模型。

> bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型

在bagging和boosting框架中，为了计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差已经两两之间的相关系数相等，由于bagging和boosting之间的基模型是线性组成的，则有：
$$
\begin{aligned}
E(F) 
&=e(\sum_{i}^mr_i*f_i)\\
&=\sum_{i}^mr_i*E(f_i)\\
&=r*\sum_{i}^mE(f_i) \\

Var(F) 
&=Var(\sum_{i}^mr_i*f_i)\\
&=Cov(\sum_{i}^mr_i*f_i， \sum_{i}^mr_i*f_i)\\
&=\sum_{i}^m r_i^2*Var(f_i)+ \sum_{i}^m\sum_{j \ne i}^m2*\rho*r_i*r_j*\sqrt{Var(f_i)}*\sqrt{Var(f_j)} \\
&=m*r^2*\sigma^2 + m^2 *r^2*\sigma^2*\rho - m*r^2*\sigma^2*\rho \\
&= m^2 *r^2*\sigma^2*\rho+m *r^2*\sigma^2*(1-\rho) \\
\end{aligned}
$$

$$
\begin{aligned}

\end{aligned}
$$

#### bagging的偏差和方差

对于bagging来说，每个基模型的权重等于$$\frac{1}{m}$$且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：
$$
\begin{aligned}
E(F) 
&=r*\sum_{i}^mE(f_i) \\
&=\frac{1}{m}*m*\mu\\
&=\mu\\

Var
&= m^2 *r^2*\sigma^2*\rho+m *r^2*\sigma^2*(1-\rho) \\
&=m^2*\frac{1}{m^2}*\sigma^2*\rho + m^2*\frac{1}{m^2}*\sigma^2*(1-\rho )\\
&=\sigma^2*\rho+\frac{\sigma^2*(1-\rho)}{m}
\end{aligned}
$$

* 整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。
* 整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。
* 当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。
* 为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。

#### boosting的偏差和方差

对于boosting来说，基模型训练集抽样是强相关的，那么模型的相关系数近似为1，则针对boosting的化简公式：
$$
\begin{aligned}
E(F) 
&= r*\sum_{i}^mE(f_i) \\
Var(F) 
&= m^2 *r^2*\sigma^2*\rho+m *r^2*\sigma^2*(1-\rho) \\
&= m^2 *r^2*\sigma^2*1+m *r^2*\sigma^2*(1-1) \\
&= m^2 *r^2*\sigma^2
\end{aligned}
$$

* 我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。
* 因为基模型为弱模型，导致了每个基模型的准确度都不是很高（即偏差较大），随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。
* 但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。

#### 模型的独立性

我们说过，抽样的随机性决定了模型的随机性，如果两个模型的训练集抽样过程不独立，则两个模型则不独立。这时便有一个天大的陷阱在等着我们：**bagging中基模型的训练样本都是独立的随机抽样，但是基模型却不独立呢**？

因为bagging的抽样是针对训练集的抽样，并不能称之为整体的独立随机抽样，详见描述[<<为什么说bagging是减少variance，而boosting是减少bias?>>](https://www.zhihu.com/question/26760839).

* 样本随机抽样：整体模型中对样本的抽样，增加m, 降低Var的第二项
* 特征随机选择：整体模型中随机抽取若干随机变量称为基模型的随机变量，降低$$\sigma$$

#### 小结

1. 使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力
2. 对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低
3. 对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低
4. 整体模型的偏差和方差与基模型的偏差和方差息息相关

### 结合策略

假定得到的T个弱学习器是$$\{h_1, h_2, \cdots, h_T\}$$

#### 平均法

对于数值类回归预测问题，通畅使用平均法，即对于若干和弱学习器的输出进行平均得到最终的预测输出。

最简单的平均即算数平均：
$$
H(x) = \frac{1}{T}\sum_{i=1}^nh_i(x)
$$
如果每个个体学习器都有一个权重w, 则最终预测是：
$$
H(x) = \sum_{i=1}^Tw_ih_i(x)
$$
其中，$$w_i$$满足：
$$
w_i \ge0, \qquad \sum_{i=1}^Tw_i=1
$$

#### 投票法

对于分类问题，通常使用投票法，假设我们的预测类别：$${c_1, c_2, \cdots c_k}$$，对于任意一个预测样本x， 我们的T个弱学习器的预测结果分别是$$\{h_1(x), h_2(x), \cdots, h_T(x)\}$$。

1. 少数服从多数原则，即T个弱学习器对样本x的预测结果中，数量最多的类别$$c_i$$为最终的分类类别
2. 绝对多数投票法，即票数过半，否则拒绝预测
3. 加权投票法，即每个弱学习器的分类票数乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别

### 参考阅读

[协方差](https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE)

[集成学习](https://zhuanlan.zhihu.com/p/27689464)

[使用sklearn进行集成学习—理论](http://www.cnblogs.com/jasonfreak/p/5657196.html)

[集成原理学习小结](http://www.cnblogs.com/pinard/p/6131423.html)

